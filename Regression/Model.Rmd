# Modelo de regresión

## Imports necesarios

```{r}
#install.packages("caret")
#install.packages("rpart")
#install.packages("lightgbm")
#install.packages("randomForest")
```

```{r}
library(randomForest)
library(caret)
library(rpart)
library(lightgbm)
```

## 1. Lectura de los datos

Lectura de los datos obtenidos tras aplicar el proceso de selección.

```{r}

df <- read.csv("../data/CleanData/datos_seleccion_atributos.csv")
control <- trainControl(method = "cv", number = 10)
```

## 2. Modelos

### 2.1. Random Forest

```{r}

model_rf <- train(Nota.media ~ ., data = df, method = "rf", trControl = control, tuneGrid = data.frame(mtry = 2), ntree = 10)
print(model_rf)
```

### 2.2. LightGBM

```{r}

data <- as.matrix(df[, -which(names(df) == "Nota.media")])
label <- df$Nota.media

# Crear un conjunto de datos de LightGBM
lgb_data <- lgb.Dataset(data = data, label = label)

# Especificar los parámetros del modelo
params <- list(objective = "regression",
               metric = list("rmse","mse", "mae"), #TODO: Ver como se puede modificar esto para que salgan todas las métricas
               num_leaves = 31,
               learning_rate = 0.05,
               nthread = 2,
               max_depth = 2,
               min_data_in_leaf = 20)

# Entrenar el modelo con validación cruzada
cv_results <- lgb.cv(params,
                     data = lgb_data,
                     nrounds = 10,
                     nfold = 10,
                     verbose = 0)

# Mostrar los resultados
print(cv_results)
```

**RMSE**

```{r}

cv_results$record_evals$valid$rmse
```

**MAE**

```{r}

cv_results$record_evals$valid$l1
```

**MSE**

```{r}

cv_results$record_evals$valid$l2
```

### 2.3. Decision Tree

```{r}

model_tree <- train(Nota.media ~ ., data = df, method = "rpart", trControl = control, control = rpart.control(cp = 1))

print(model_tree)

```
